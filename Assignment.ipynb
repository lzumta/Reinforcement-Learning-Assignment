{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzumta/Reinforcement-Learning-Assignment/blob/main/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02944396",
      "metadata": {
        "id": "02944396"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c9652bc6",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "c9652bc6"
      },
      "outputs": [],
      "source": [
        "# Import \n",
        "\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "from decimal import Decimal\n",
        "from degree_freedom_queen import *\n",
        "from degree_freedom_king1 import *\n",
        "from degree_freedom_king2 import *\n",
        "from generate_game import *\n",
        "from Chess_env import *\n",
        "import time\n",
        "\n",
        "\n",
        "size_board = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bceca7c",
      "metadata": {
        "id": "0bceca7c"
      },
      "source": [
        "## The Environment\n",
        "\n",
        "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
        "Chess_env is composed by the following methods:\n",
        "\n",
        "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
        "\n",
        "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
        "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
        "     \n",
        "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
        "     \n",
        "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
        "     \n",
        "\n",
        "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
        "\n",
        "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
        "     \n",
        "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
        "     \n",
        "     \n",
        "3. Features. Given the chessboard position, the method computes the features.\n",
        "\n",
        "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9593a299",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "9593a299"
      },
      "outputs": [],
      "source": [
        "## INITIALISE THE ENVIRONMENT\n",
        "\n",
        "env=Chess_Env(size_board)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bbc05bfd",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "bbc05bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faca481-55fc-4f3b-88d7-1b5e2d99b691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 1 0]\n",
            " [3 0 0 0]]\n",
            "check?  0\n",
            "dofk2  0\n",
            "\n",
            "[[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 2 1 0]\n",
            " [3 0 0 0]]\n",
            "1  1\n",
            "-----------------\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "check?  1\n",
            "dofk2  0\n"
          ]
        }
      ],
      "source": [
        "## PRINT 5 STEPS OF AN EPISODE CONSIDERING A RANDOM AGENT\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()                       # INTIALISE GAME\n",
        "\n",
        "print(S)                                                  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)\n",
        "\n",
        "print('check? ',env.check)                                # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)\n",
        "print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))    # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    a,_=np.where(allowed_a==1)                  # FIND WHAT THE ALLOWED ACTIONS ARE\n",
        "    #np.random.seed(42)\n",
        "    a_agent=np.random.permutation(a)[0]         # MAKE A RANDOM ACTION\n",
        "\n",
        "    S,X,allowed_a,R,Done=env.OneStep(a_agent)   # UPDATE THE ENVIRONMENT\n",
        "    \n",
        "    \n",
        "    ## PRINT CHESS BOARD AND VARIABLES\n",
        "    print('')\n",
        "    print(S)\n",
        "    print(R,'', Done)\n",
        "    print('-----------------')\n",
        "    print(X)\n",
        "    #print(allowed_a)\n",
        "    print('check? ',env.check)\n",
        "    print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))\n",
        "    \n",
        "    \n",
        "    # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)\n",
        "    if Done:\n",
        "        break\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc16cf7b",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "fc16cf7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124fa55b-e0b9-47f3-eac0-b1f124d89c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random_Agent, Average reward: -0.628 Number of steps:  7.151\n"
          ]
        }
      ],
      "source": [
        "# PERFORM N_episodes=1000 EPISODES MAKING RANDOM ACTIONS AND COMPUTE THE AVERAGE REWARD AND NUMBER OF MOVES \n",
        "np.random.seed(42)\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "N_episodes=1000\n",
        "\n",
        "# VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES \n",
        "R_save_random = np.zeros([N_episodes, 1])\n",
        "N_moves_save_random = np.zeros([N_episodes, 1])\n",
        "\n",
        "for n in range(N_episodes):\n",
        "\n",
        "    if n % 500 == 0:\n",
        "      print('Average reward:',np.mean(R_save_random[n-500:n]),'Number of steps: ',np.mean(N_moves_save_random[n-500:n]))\n",
        "    \n",
        "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
        "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
        "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
        "\n",
        "    \n",
        "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
        "    while Done==0:\n",
        "        \n",
        "        # SAME AS THE CELL BEFORE, BUT SAVING THE RESULTS WHEN THE EPISODE TERMINATES \n",
        "        \n",
        "        a,_=np.where(allowed_a==1)\n",
        "        a_agent=np.random.permutation(a)[0]\n",
        "\n",
        "        S,X,allowed_a,R,Done=env.OneStep(a_agent)\n",
        "        \n",
        "        \n",
        "        if Done:\n",
        "            R_save_random[n]=np.copy(R)\n",
        "            N_moves_save_random[n]=np.copy(i)\n",
        "            break\n",
        "\n",
        "        i=i+1                               # UPDATE THE COUNTER\n",
        "\n",
        "\n",
        "\n",
        "# AS YOU SEE, THE PERFORMANCE OF A RANDOM AGENT ARE NOT GREAT, SINCE THE MAJORITY OF THE POSITIONS END WITH A DRAW \n",
        "# (THE ENEMY KING IS NOT IN CHECK AND CAN'T MOVE)\n",
        "\n",
        "print('Random_Agent, Average reward:',np.mean(R_save_random),'Number of steps: ',np.mean(N_moves_save_random))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# moving average\n",
        "def moving_average(a, n) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "#print(moving_average(R_save, n=500))"
      ],
      "metadata": {
        "id": "S2c9WlZtlKqA"
      },
      "id": "S2c9WlZtlKqA",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "fig, axs = plt.subplots(2,figsize=(15,10))\n",
        "\n",
        "axs[0].plot(np.arange(0,N_episodes), R_save_random, 'b')\n",
        "axs[1].plot(np.arange(0,N_episodes-1999), moving_average(R_save_random, n=2000), 'r')\n",
        "axs[0].set_title(\"Reward per game\")\n",
        "axs[1].set_title(\"Exponential moving average\")\n",
        "plt.xticks([])"
      ],
      "metadata": {
        "id": "OCluRLg7hjHY"
      },
      "id": "OCluRLg7hjHY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.rcParams[\"figure.figsize\"] = (10,7.5)\n",
        "\n",
        "plt.plot(np.arange(0,N_episodes),N_moves_save_random,'g')\n",
        "plt.legend('Number of moves')\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Number of moves')"
      ],
      "metadata": {
        "id": "nXKI3cwHitLz"
      },
      "id": "nXKI3cwHitLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "ece20429",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "ece20429"
      },
      "outputs": [],
      "source": [
        "# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...\n",
        "# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...\n",
        "# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200. \n",
        "\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "\n",
        "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
        "N_h=200                ## NUMBER OF HIDDEN NODES\n",
        "\n",
        "memory_size = 2000\n",
        "\n",
        "## INITALISE YOUR NEURAL NETWORK...\n",
        "class Neural_Netork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Neural_Netork, self).__init__()\n",
        "        self.fc1 = nn.Linear(N_in, N_h)\n",
        "        self.out = nn.Linear(N_h, N_a)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        action = self.out(x)\n",
        "        return action\n",
        "\n",
        "\n",
        "# HYPERPARAMETERS SUGGESTED (FOR A GRID SIZE OF 4)\n",
        "\n",
        "epsilon_0 = 0.6    # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
        "beta = 0.00005      # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
        "gamma = 0.85        # THE DISCOUNT FACTOR\n",
        "eta = 0.01        # THE LEARNING RATE\n",
        "\n",
        "N_episodes = 100000 # THE NUMBER OF GAMES TO BE PLAYED \n",
        "\n",
        "# SAVING VARIABLES\n",
        "R_save = np.zeros([N_episodes, 1])\n",
        "N_moves_save = np.zeros([N_episodes, 1])\n",
        "Training_time_per_episode = np.zeros([N_episodes, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChessAgent():\n",
        "    def __init__(self):\n",
        "        super(ChessAgent, self).__init__()\n",
        "        self.policy_net = Neural_Netork()\n",
        "        self.target_net = Neural_Netork()\n",
        "\n",
        "        self.learn_step_counter = 0\n",
        "        self.memory_counter = 0\n",
        "        self.memory_q = np.zeros((memory_size, N_in * 2 + 2))\n",
        "        self.memory_sarsa = np.zeros((memory_size, N_in * 2 + 4))\n",
        "        \n",
        "        self.optimizer = torch.optim.SGD(self.policy_net.parameters(), lr=eta)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "    def choose_action(self, state, allowed_a, epsilon_f):\n",
        "        state = torch.unsqueeze(torch.FloatTensor(state), 0) # get a 1D array\n",
        "        a,_=np.where(allowed_a==1) # get indexes of allowed actions\n",
        "        if np.random.random() > epsilon_f: # greedy policy\n",
        "            # Exploitation\n",
        "            action_value = self.policy_net.forward(state)\n",
        "            actions = torch.detach(action_value).numpy()\n",
        "            actions = [actions[0][i] for i in a]\n",
        "            action = np.argmax(actions)\n",
        "            action = a[action]\n",
        "        else: # random policy\n",
        "            # Exploration\n",
        "            action = np.random.permutation(a)[0]\n",
        "        return action\n",
        "\n",
        "\n",
        "    def store_transition_q(self, state, action, reward, next_state):\n",
        "        transition = np.hstack((state, [action, reward], next_state))\n",
        "        index = self.memory_counter % memory_size\n",
        "        self.memory_q[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def store_transition_sarsa(self, state, action1, action2, reward, next_state, done):\n",
        "        transition = np.hstack((state, [action1, action2, reward, done], next_state))\n",
        "        index = self.memory_counter % memory_size\n",
        "        self.memory_sarsa[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def train_q(self):\n",
        "\n",
        "        #update the parameters\n",
        "        if self.learn_step_counter % 100 ==0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.learn_step_counter+=1\n",
        "\n",
        "        #sample batch from memory\n",
        "        sample_index = np.random.choice(memory_size, 32)\n",
        "        batch_memory = self.memory_q[sample_index, :]\n",
        "        batch_state = torch.FloatTensor(batch_memory[:, :N_in])\n",
        "        batch_action = torch.LongTensor(batch_memory[:, N_in:N_in+1].astype(int))\n",
        "        batch_reward = torch.FloatTensor(batch_memory[:, N_in+1:N_in+2])\n",
        "        batch_next_state = torch.FloatTensor(batch_memory[:,-N_in:])\n",
        "\n",
        "        #q_eval\n",
        "        q_eval = self.policy_net(batch_state).gather(1, batch_action)\n",
        "        q_next = self.target_net(batch_next_state).detach()\n",
        "        q_target = batch_reward + gamma * q_next.max(1)[0].view(32, 1)\n",
        "        loss = self.loss_function(q_eval, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def train_sarsa(self):\n",
        "        #update the parameters\n",
        "        if self.learn_step_counter % 100 ==0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.learn_step_counter+=1\n",
        "\n",
        "        #sample batch from memory\n",
        "        sample_index = np.random.choice(memory_size, 32)\n",
        "        batch_memory = self.memory_sarsa[sample_index, :]\n",
        "        batch_state = torch.FloatTensor(batch_memory[:, :N_in])\n",
        "        batch_action1 = torch.LongTensor(batch_memory[:, N_in:N_in+1].astype(int))\n",
        "        batch_action2 = torch.LongTensor(batch_memory[:, N_in+1:N_in+2].astype(int))\n",
        "        batch_reward = torch.FloatTensor(batch_memory[:, N_in+2:N_in+3])\n",
        "        batch_done = torch.FloatTensor(batch_memory[:, N_in+3:N_in+4])\n",
        "        batch_next_state = torch.FloatTensor(batch_memory[:,-N_in:])\n",
        "\n",
        "        #q_eval\n",
        "        q_eval = self.policy_net(batch_state).gather(1, batch_action1)          \n",
        "        q_next = self.target_net(batch_next_state).gather(1, batch_action2)\n",
        "        q_target = batch_reward + gamma * q_next\n",
        "        loss = self.loss_function(q_eval, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "ZDdj5p37FgKV"
      },
      "id": "ZDdj5p37FgKV",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ba1f84",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "e6ba1f84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f80e57-3834-46ae-9279-ab8b48364624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: -0.556 Number of steps:  7.47\n",
            "Average reward: -0.196 Number of steps:  8.71\n",
            "Average reward: -0.06 Number of steps:  8.82\n",
            "Average reward: 0.176 Number of steps:  6.214\n",
            "Average reward: 0.176 Number of steps:  5.598\n",
            "Average reward: 0.396 Number of steps:  5.434\n",
            "Average reward: 0.344 Number of steps:  4.746\n",
            "Average reward: 0.396 Number of steps:  4.714\n",
            "Average reward: 0.468 Number of steps:  4.63\n",
            "Average reward: 0.46 Number of steps:  4.928\n",
            "Average reward: 0.492 Number of steps:  4.628\n",
            "Average reward: 0.372 Number of steps:  4.408\n",
            "Average reward: 0.528 Number of steps:  4.668\n",
            "Average reward: 0.512 Number of steps:  4.28\n",
            "Average reward: 0.444 Number of steps:  4.016\n",
            "Average reward: 0.52 Number of steps:  3.824\n",
            "Average reward: 0.444 Number of steps:  3.842\n",
            "Average reward: 0.564 Number of steps:  4.056\n",
            "Average reward: 0.564 Number of steps:  4.482\n",
            "Average reward: 0.572 Number of steps:  3.8\n",
            "Average reward: 0.536 Number of steps:  4.044\n",
            "Average reward: 0.524 Number of steps:  3.634\n",
            "Average reward: 0.54 Number of steps:  3.826\n",
            "Average reward: 0.504 Number of steps:  3.48\n",
            "Average reward: 0.616 Number of steps:  3.728\n",
            "Average reward: 0.6 Number of steps:  3.436\n",
            "Average reward: 0.624 Number of steps:  3.726\n",
            "Average reward: 0.608 Number of steps:  3.492\n",
            "Average reward: 0.656 Number of steps:  3.606\n",
            "Average reward: 0.648 Number of steps:  3.348\n",
            "Average reward: 0.664 Number of steps:  3.474\n",
            "Average reward: 0.716 Number of steps:  3.53\n",
            "Average reward: 0.704 Number of steps:  3.3\n",
            "Average reward: 0.652 Number of steps:  3.408\n",
            "Average reward: 0.636 Number of steps:  3.376\n",
            "Average reward: 0.68 Number of steps:  3.298\n",
            "Average reward: 0.696 Number of steps:  3.168\n",
            "Average reward: 0.644 Number of steps:  3.168\n",
            "Average reward: 0.716 Number of steps:  3.484\n",
            "Average reward: 0.668 Number of steps:  3.174\n",
            "Average reward: 0.688 Number of steps:  3.15\n",
            "Average reward: 0.756 Number of steps:  3.298\n",
            "Average reward: 0.688 Number of steps:  3.2\n",
            "Average reward: 0.696 Number of steps:  2.926\n",
            "Average reward: 0.72 Number of steps:  3.154\n",
            "Average reward: 0.7 Number of steps:  2.9\n",
            "Average reward: 0.708 Number of steps:  3.008\n",
            "Average reward: 0.736 Number of steps:  3.232\n",
            "Average reward: 0.724 Number of steps:  3.122\n",
            "Average reward: 0.688 Number of steps:  3.068\n",
            "Average reward: 0.736 Number of steps:  3.114\n",
            "Average reward: 0.676 Number of steps:  2.94\n",
            "Average reward: 0.644 Number of steps:  3.066\n",
            "Average reward: 0.736 Number of steps:  3.058\n",
            "Average reward: 0.748 Number of steps:  2.958\n",
            "Average reward: 0.724 Number of steps:  3.272\n",
            "Average reward: 0.732 Number of steps:  3.058\n",
            "Average reward: 0.784 Number of steps:  2.838\n",
            "Average reward: 0.788 Number of steps:  2.92\n",
            "Average reward: 0.844 Number of steps:  2.878\n",
            "Average reward: 0.76 Number of steps:  2.738\n",
            "Average reward: 0.792 Number of steps:  2.914\n",
            "Average reward: 0.832 Number of steps:  2.79\n",
            "Average reward: 0.776 Number of steps:  2.962\n",
            "Average reward: 0.804 Number of steps:  2.932\n",
            "Average reward: 0.82 Number of steps:  2.97\n",
            "Average reward: 0.8 Number of steps:  2.926\n",
            "Average reward: 0.836 Number of steps:  2.902\n",
            "Average reward: 0.808 Number of steps:  2.828\n",
            "Average reward: 0.812 Number of steps:  2.904\n",
            "Average reward: 0.772 Number of steps:  2.75\n",
            "Average reward: 0.784 Number of steps:  2.72\n",
            "Average reward: 0.812 Number of steps:  2.686\n",
            "Average reward: 0.788 Number of steps:  2.666\n",
            "Average reward: 0.816 Number of steps:  2.686\n",
            "Average reward: 0.824 Number of steps:  2.576\n",
            "Average reward: 0.796 Number of steps:  2.684\n",
            "Average reward: 0.796 Number of steps:  2.738\n",
            "Average reward: 0.828 Number of steps:  2.72\n",
            "Average reward: 0.788 Number of steps:  2.728\n",
            "Average reward: 0.872 Number of steps:  2.674\n",
            "Average reward: 0.836 Number of steps:  2.682\n",
            "Average reward: 0.844 Number of steps:  2.71\n",
            "Average reward: 0.808 Number of steps:  2.694\n",
            "Average reward: 0.836 Number of steps:  2.788\n",
            "Average reward: 0.812 Number of steps:  2.818\n",
            "Average reward: 0.804 Number of steps:  2.782\n",
            "Average reward: 0.892 Number of steps:  2.706\n",
            "Average reward: 0.848 Number of steps:  2.738\n",
            "Average reward: 0.86 Number of steps:  2.68\n",
            "Average reward: 0.848 Number of steps:  2.65\n",
            "Average reward: 0.828 Number of steps:  2.704\n",
            "Average reward: 0.808 Number of steps:  2.816\n",
            "Average reward: 0.852 Number of steps:  2.628\n",
            "Average reward: 0.82 Number of steps:  2.514\n",
            "Average reward: 0.848 Number of steps:  2.546\n",
            "Average reward: 0.864 Number of steps:  2.644\n",
            "Average reward: 0.868 Number of steps:  2.672\n",
            "Average reward: 0.856 Number of steps:  2.614\n",
            "Average reward: 0.848 Number of steps:  2.734\n",
            "Average reward: 0.824 Number of steps:  2.588\n",
            "Average reward: 0.856 Number of steps:  2.634\n",
            "Average reward: 0.852 Number of steps:  2.698\n",
            "Average reward: 0.84 Number of steps:  2.646\n",
            "Average reward: 0.844 Number of steps:  2.654\n",
            "Average reward: 0.832 Number of steps:  2.42\n",
            "Average reward: 0.868 Number of steps:  2.66\n",
            "Average reward: 0.86 Number of steps:  2.65\n",
            "Average reward: 0.892 Number of steps:  2.49\n",
            "Average reward: 0.864 Number of steps:  2.548\n",
            "Average reward: 0.864 Number of steps:  2.604\n",
            "Average reward: 0.908 Number of steps:  2.496\n",
            "Average reward: 0.844 Number of steps:  2.724\n",
            "Average reward: 0.84 Number of steps:  2.542\n",
            "Average reward: 0.872 Number of steps:  2.584\n",
            "Average reward: 0.872 Number of steps:  2.794\n",
            "Average reward: 0.884 Number of steps:  2.418\n",
            "Average reward: 0.856 Number of steps:  2.628\n",
            "Average reward: 0.888 Number of steps:  2.514\n",
            "Average reward: 0.88 Number of steps:  2.6\n",
            "Average reward: 0.876 Number of steps:  2.588\n",
            "Average reward: 0.892 Number of steps:  2.608\n",
            "Average reward: 0.884 Number of steps:  2.648\n",
            "Average reward: 0.9 Number of steps:  2.478\n",
            "Average reward: 0.9 Number of steps:  2.636\n",
            "Average reward: 0.888 Number of steps:  2.472\n",
            "Average reward: 0.892 Number of steps:  2.58\n",
            "Average reward: 0.888 Number of steps:  2.606\n",
            "Average reward: 0.888 Number of steps:  2.658\n",
            "Average reward: 0.876 Number of steps:  2.518\n",
            "Average reward: 0.9 Number of steps:  2.696\n",
            "Average reward: 0.88 Number of steps:  2.74\n",
            "Average reward: 0.912 Number of steps:  2.6\n",
            "Average reward: 0.908 Number of steps:  2.542\n",
            "Average reward: 0.892 Number of steps:  2.49\n",
            "Average reward: 0.892 Number of steps:  2.614\n",
            "Average reward: 0.908 Number of steps:  2.55\n",
            "Average reward: 0.876 Number of steps:  2.672\n",
            "Average reward: 0.924 Number of steps:  2.662\n",
            "Average reward: 0.88 Number of steps:  2.51\n",
            "Average reward: 0.888 Number of steps:  2.65\n",
            "Average reward: 0.888 Number of steps:  2.394\n",
            "Average reward: 0.896 Number of steps:  2.552\n",
            "Average reward: 0.936 Number of steps:  2.452\n",
            "Average reward: 0.896 Number of steps:  2.564\n",
            "Average reward: 0.868 Number of steps:  2.606\n",
            "Average reward: 0.932 Number of steps:  2.554\n",
            "Average reward: 0.904 Number of steps:  2.484\n",
            "Average reward: 0.868 Number of steps:  2.332\n",
            "Average reward: 0.928 Number of steps:  2.44\n",
            "Average reward: 0.904 Number of steps:  2.492\n",
            "Average reward: 0.9 Number of steps:  2.608\n",
            "Average reward: 0.928 Number of steps:  2.554\n",
            "Average reward: 0.916 Number of steps:  2.468\n",
            "Average reward: 0.884 Number of steps:  2.542\n",
            "Average reward: 0.92 Number of steps:  2.356\n",
            "Average reward: 0.896 Number of steps:  2.514\n",
            "Average reward: 0.908 Number of steps:  2.374\n",
            "Average reward: 0.892 Number of steps:  2.358\n",
            "Average reward: 0.912 Number of steps:  2.608\n",
            "Average reward: 0.92 Number of steps:  2.518\n",
            "Average reward: 0.912 Number of steps:  2.552\n",
            "Average reward: 0.904 Number of steps:  2.55\n",
            "Average reward: 0.904 Number of steps:  2.538\n",
            "Average reward: 0.908 Number of steps:  2.388\n",
            "Average reward: 0.944 Number of steps:  2.428\n",
            "Average reward: 0.904 Number of steps:  2.398\n",
            "Average reward: 0.888 Number of steps:  2.456\n",
            "Average reward: 0.928 Number of steps:  2.348\n",
            "Average reward: 0.912 Number of steps:  2.406\n",
            "Average reward: 0.952 Number of steps:  2.422\n",
            "Average reward: 0.908 Number of steps:  2.61\n",
            "Average reward: 0.92 Number of steps:  2.382\n",
            "Average reward: 0.908 Number of steps:  2.4\n",
            "Average reward: 0.916 Number of steps:  2.334\n",
            "Average reward: 0.928 Number of steps:  2.33\n",
            "Average reward: 0.94 Number of steps:  2.42\n",
            "Average reward: 0.904 Number of steps:  2.34\n",
            "Average reward: 0.896 Number of steps:  2.296\n",
            "Average reward: 0.9 Number of steps:  2.486\n",
            "Average reward: 0.896 Number of steps:  2.43\n",
            "Average reward: 0.916 Number of steps:  2.402\n",
            "Average reward: 0.932 Number of steps:  2.414\n",
            "Average reward: 0.888 Number of steps:  2.482\n",
            "Average reward: 0.912 Number of steps:  2.46\n",
            "Average reward: 0.92 Number of steps:  2.406\n",
            "Average reward: 0.892 Number of steps:  2.428\n",
            "Average reward: 0.928 Number of steps:  2.46\n",
            "Average reward: 0.908 Number of steps:  2.508\n",
            "Average reward: 0.932 Number of steps:  2.346\n",
            "Average reward: 0.928 Number of steps:  2.476\n",
            "Average reward: 0.92 Number of steps:  2.408\n",
            "Average reward: 0.928 Number of steps:  2.572\n",
            "Average reward: 0.888 Number of steps:  2.512\n",
            "Average reward: 0.924 Number of steps:  2.37\n",
            "Average reward: 0.888 Number of steps:  2.554\n",
            "Average reward: 0.916 Number of steps:  2.402\n",
            "Average reward: 0.884 Number of steps:  2.632\n",
            "Average reward: 0.94 Number of steps:  2.488\n"
          ]
        }
      ],
      "source": [
        "# TRAINING LOOP BONE STRUCTURE...\n",
        "# I WROTE FOR YOU A RANDOM AGENT (THE RANDOM AGENT WILL BE SLOWER TO GIVE CHECKMATE THAN AN OPTIMISED ONE, \n",
        "# SO DON'T GET CONCERNED BY THE TIME IT TAKES), CHANGE WITH YOURS ...\n",
        "agent = ChessAgent()\n",
        "update_every = 2\n",
        "\n",
        "for n in range(N_episodes):\n",
        "\n",
        "    if n % 500 == 0 and n != 0:\n",
        "      print('Average reward:',np.mean(R_save[n-500:n]),'Number of steps: ',np.mean(N_moves_save[n-500:n]), 'Training time: ',np.mean(Training_time_per_episode[n-500:n]))\n",
        "\n",
        "    start = time.time()\n",
        "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
        "    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
        "    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS\n",
        "    \n",
        "    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME\n",
        "    #print(n)                                 ## REMOVE THIS OF COURSE, WE USED THIS TO CHECK THAT IT WAS RUNNING\n",
        "\n",
        "    ep_reward = 0\n",
        "    \n",
        "    while Done==0:                           ## START THE EPISODE\n",
        "        \n",
        "        \n",
        "        ## THIS IS A RANDOM AGENT, CHANGE IT...\n",
        "        action = agent.choose_action(X, allowed_a, epsilon_f)\n",
        "\n",
        "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(action)\n",
        "        agent.store_transition_q(X, action, R, X_next)\n",
        "\n",
        "        if agent.memory_counter >= memory_size:\n",
        "                agent.train_q()      \n",
        "\n",
        "        ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE     \n",
        "        if Done==1:\n",
        "          R_save[n]=np.copy(R)\n",
        "          N_moves_save[n]=np.copy(i)\n",
        "          end = time.time()\n",
        "          Training_time_per_episode[n] = abs(start-end)/1000\n",
        "          break\n",
        "\n",
        "        # IF THE EPISODE IS NOT OVER...\n",
        "        else:\n",
        "            \n",
        "          ## ONLY TO PUT SUMETHING\n",
        "          ep_reward += R\n",
        "            \n",
        "            \n",
        "        # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
        "        S=np.copy(S_next)\n",
        "        X=np.copy(X_next)\n",
        "        allowed_a=np.copy(allowed_a_next)\n",
        "        \n",
        "        i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
        "    \n",
        "           "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Trained_Agent, Average reward:',np.mean(R_save),'Number of steps: ',np.mean(N_moves_save))"
      ],
      "metadata": {
        "id": "aBwFIOK8SfzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71c4c1e-0240-4b81-a2a4-f8b121c33265"
      },
      "id": "aBwFIOK8SfzR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained_Agent, Average reward: 0.76402 Number of steps:  3.27898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot: Reward per game\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "fig, axs = plt.subplots(2,figsize=(15,10))\n",
        "\n",
        "axs[0].plot(np.arange(0,N_episodes), R_save, 'b')\n",
        "axs[1].plot(np.arange(0,N_episodes-1999), moving_average(R_save, n=2000), 'r')\n",
        "axs[0].set_title(\"Reward per game\")\n",
        "axs[1].set_title(\"Exponential moving average\")\n",
        "plt.xticks([])"
      ],
      "metadata": {
        "id": "zM2uEmr_jlgP"
      },
      "id": "zM2uEmr_jlgP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot: Number of moves per game vs training time\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
        "\n",
        "plt.plot(np.arange(0,N_episodes), N_moves_save, 'g')\n",
        "plt.ylabel('Number of moves')\n",
        "plt.xlabel('Number of games')"
      ],
      "metadata": {
        "id": "b-vz-fulsv9j"
      },
      "id": "b-vz-fulsv9j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q-Learning is an example of model-free learning algorithm. It does not assume that agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error.\n",
        "The basic idea of Q-Learning is to approximate the state-action pairs Q-function from the samples of Q(s, a) that we observe during interaction with the enviornment. This approach is known as Time-Difference Learning."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "IKtDM6m--bLn"
      },
      "id": "IKtDM6m--bLn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep SARSA Implementation"
      ],
      "metadata": {
        "id": "gMfsh611oeGg"
      },
      "id": "gMfsh611oeGg"
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING LOOP BONE STRUCTURE...\n",
        "# I WROTE FOR YOU A RANDOM AGENT (THE RANDOM AGENT WILL BE SLOWER TO GIVE CHECKMATE THAN AN OPTIMISED ONE, \n",
        "# SO DON'T GET CONCERNED BY THE TIME IT TAKES), CHANGE WITH YOURS ...\n",
        "agent = ChessAgent()\n",
        "update_every = 2\n",
        "\n",
        "for n in range(N_episodes):\n",
        "\n",
        "    if n % 500 == 0 and n != 0:\n",
        "      print('Average reward:',np.mean(R_save[n-500:n]),'Number of steps: ',np.mean(N_moves_save[n-500:n]), 'Training time: ',np.mean(Training_time_per_episode[n-500:n]))\n",
        "\n",
        "    start = time.time()\n",
        "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
        "    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
        "    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS\n",
        "    \n",
        "    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME\n",
        "    #print(n)                                 ## REMOVE THIS OF COURSE, WE USED THIS TO CHECK THAT IT WAS RUNNING\n",
        "\n",
        "    action1 = agent.choose_action(X, allowed_a, epsilon_f)\n",
        "\n",
        "    ep_reward = 0\n",
        "    \n",
        "    while Done==0:                           ## START THE EPISODE\n",
        "        \n",
        "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(action1)\n",
        "\n",
        "        if agent.memory_counter >= memory_size:\n",
        "              agent.train_sarsa()  \n",
        "\n",
        "        if Done==1: \n",
        "          action2 = 0  # just to add something to the memory       \n",
        "          agent.store_transition_sarsa(X, action1, action2, R, X_next, Done)\n",
        "          R_save[n]=np.copy(R)\n",
        "          N_moves_save[n]=np.copy(i)\n",
        "          end = time.time()\n",
        "          Training_time_per_episode[n] = (abs(start-end)/1000)\n",
        "          break\n",
        "        \n",
        "        action2 = agent.choose_action(X_next, allowed_a_next, epsilon_f)\n",
        "        \n",
        "        agent.store_transition_sarsa(X, action1, action2, R, X_next, Done)\n",
        "\n",
        "       \n",
        "\n",
        "        ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE     \n",
        "        if Done==1:\n",
        "          R_save[n]=np.copy(R)\n",
        "          N_moves_save[n]=np.copy(i)\n",
        "          end = time.time()\n",
        "          Training_time_per_episode[n] = (abs(start-end)/1000)\n",
        "          break\n",
        "\n",
        "        # IF THE EPISODE IS NOT OVER...\n",
        "        else:\n",
        "            \n",
        "          ## ONLY TO PUT SUMETHING\n",
        "          ep_reward += R\n",
        "            \n",
        "            \n",
        "        # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
        "        S=np.copy(S_next)\n",
        "        X=np.copy(X_next)\n",
        "        action1=np.copy(action2)\n",
        "        allowed_a=np.copy(allowed_a_next)\n",
        "        \n",
        "        i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
        "  "
      ],
      "metadata": {
        "id": "BI4xo7UZoj-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd03f15b-9c0b-4ae5-e4e3-6104e5520387"
      },
      "id": "BI4xo7UZoj-O",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: -0.448 Number of steps:  8.002 Training time:  1.3997572422027586e-05\n",
            "Average reward: -0.184 Number of steps:  9.014 Training time:  2.074413728713989e-05\n",
            "Average reward: -0.116 Number of steps:  8.918 Training time:  2.0667477607727053e-05\n",
            "Average reward: -0.04 Number of steps:  8.864 Training time:  2.071912384033203e-05\n",
            "Average reward: 0.108 Number of steps:  7.726 Training time:  1.8038192749023433e-05\n",
            "Average reward: 0.196 Number of steps:  7.024 Training time:  1.6458947658538817e-05\n",
            "Average reward: 0.22 Number of steps:  6.028 Training time:  1.423497724533081e-05\n",
            "Average reward: 0.32 Number of steps:  5.866 Training time:  1.3679076671600342e-05\n",
            "Average reward: 0.428 Number of steps:  4.87 Training time:  1.1557361602783205e-05\n",
            "Average reward: 0.408 Number of steps:  4.726 Training time:  1.1040649890899658e-05\n",
            "Average reward: 0.492 Number of steps:  4.712 Training time:  1.0925305366516114e-05\n",
            "Average reward: 0.52 Number of steps:  4.676 Training time:  1.1011404991149903e-05\n",
            "Average reward: 0.484 Number of steps:  4.382 Training time:  1.0408094406127929e-05\n",
            "Average reward: 0.556 Number of steps:  4.124 Training time:  9.731497287750247e-06\n",
            "Average reward: 0.512 Number of steps:  3.776 Training time:  8.886740684509277e-06\n",
            "Average reward: 0.524 Number of steps:  4.096 Training time:  9.607833385467528e-06\n",
            "Average reward: 0.62 Number of steps:  3.718 Training time:  8.785005092620849e-06\n",
            "Average reward: 0.552 Number of steps:  3.588 Training time:  8.446633338928223e-06\n",
            "Average reward: 0.524 Number of steps:  3.654 Training time:  8.505928039550781e-06\n",
            "Average reward: 0.568 Number of steps:  3.648 Training time:  8.539839267730714e-06\n",
            "Average reward: 0.604 Number of steps:  3.646 Training time:  8.550287246704102e-06\n",
            "Average reward: 0.588 Number of steps:  3.752 Training time:  8.880036830902099e-06\n",
            "Average reward: 0.624 Number of steps:  3.696 Training time:  8.76858425140381e-06\n",
            "Average reward: 0.588 Number of steps:  3.364 Training time:  8.145967960357666e-06\n",
            "Average reward: 0.584 Number of steps:  3.43 Training time:  8.191566944122317e-06\n",
            "Average reward: 0.648 Number of steps:  3.564 Training time:  8.843357563018798e-06\n",
            "Average reward: 0.692 Number of steps:  3.688 Training time:  9.030516624450683e-06\n",
            "Average reward: 0.66 Number of steps:  3.67 Training time:  9.077128887176515e-06\n",
            "Average reward: 0.652 Number of steps:  3.284 Training time:  8.29793882369995e-06\n",
            "Average reward: 0.716 Number of steps:  3.476 Training time:  8.423500537872315e-06\n",
            "Average reward: 0.684 Number of steps:  3.18 Training time:  7.968760013580323e-06\n",
            "Average reward: 0.668 Number of steps:  3.332 Training time:  8.062002182006835e-06\n",
            "Average reward: 0.752 Number of steps:  3.348 Training time:  8.324252605438232e-06\n",
            "Average reward: 0.708 Number of steps:  3.344 Training time:  8.345149517059326e-06\n",
            "Average reward: 0.784 Number of steps:  3.4 Training time:  8.544859409332275e-06\n",
            "Average reward: 0.676 Number of steps:  3.286 Training time:  8.209750175476074e-06\n",
            "Average reward: 0.708 Number of steps:  3.24 Training time:  8.16114044189453e-06\n",
            "Average reward: 0.756 Number of steps:  3.472 Training time:  8.538358688354492e-06\n",
            "Average reward: 0.736 Number of steps:  3.226 Training time:  7.870916843414306e-06\n",
            "Average reward: 0.728 Number of steps:  3.286 Training time:  7.954358577728271e-06\n",
            "Average reward: 0.708 Number of steps:  3.148 Training time:  7.61381483078003e-06\n",
            "Average reward: 0.744 Number of steps:  3.206 Training time:  7.885845184326172e-06\n",
            "Average reward: 0.732 Number of steps:  2.88 Training time:  7.0199322700500485e-06\n",
            "Average reward: 0.72 Number of steps:  3.326 Training time:  8.071155548095705e-06\n",
            "Average reward: 0.716 Number of steps:  3.188 Training time:  7.690269947052002e-06\n",
            "Average reward: 0.76 Number of steps:  3.14 Training time:  7.941551685333251e-06\n",
            "Average reward: 0.732 Number of steps:  3.28 Training time:  8.076459407806397e-06\n",
            "Average reward: 0.792 Number of steps:  2.958 Training time:  7.2576656341552735e-06\n",
            "Average reward: 0.776 Number of steps:  3.286 Training time:  7.991779804229736e-06\n",
            "Average reward: 0.8 Number of steps:  3.142 Training time:  7.669426441192626e-06\n",
            "Average reward: 0.8 Number of steps:  2.908 Training time:  7.056615829467773e-06\n",
            "Average reward: 0.736 Number of steps:  2.962 Training time:  7.298694133758545e-06\n",
            "Average reward: 0.832 Number of steps:  3.012 Training time:  7.394458293914795e-06\n",
            "Average reward: 0.812 Number of steps:  3.012 Training time:  7.415523529052734e-06\n",
            "Average reward: 0.8 Number of steps:  2.918 Training time:  7.1967310905456535e-06\n",
            "Average reward: 0.812 Number of steps:  3.08 Training time:  7.626160144805907e-06\n",
            "Average reward: 0.832 Number of steps:  3.132 Training time:  7.5662684440612805e-06\n",
            "Average reward: 0.804 Number of steps:  2.918 Training time:  7.252227306365966e-06\n",
            "Average reward: 0.808 Number of steps:  2.95 Training time:  7.230823516845704e-06\n",
            "Average reward: 0.82 Number of steps:  2.862 Training time:  7.0600590705871585e-06\n",
            "Average reward: 0.812 Number of steps:  2.948 Training time:  7.1075053215026855e-06\n",
            "Average reward: 0.824 Number of steps:  2.896 Training time:  7.1207227706909185e-06\n",
            "Average reward: 0.844 Number of steps:  2.716 Training time:  6.8067016601562506e-06\n",
            "Average reward: 0.84 Number of steps:  2.776 Training time:  6.914040565490723e-06\n",
            "Average reward: 0.792 Number of steps:  2.742 Training time:  6.847390651702881e-06\n",
            "Average reward: 0.808 Number of steps:  2.796 Training time:  6.85421371459961e-06\n",
            "Average reward: 0.82 Number of steps:  3.036 Training time:  7.380764484405518e-06\n",
            "Average reward: 0.816 Number of steps:  2.902 Training time:  6.9865031242370615e-06\n",
            "Average reward: 0.844 Number of steps:  2.994 Training time:  7.283609867095947e-06\n",
            "Average reward: 0.848 Number of steps:  3.014 Training time:  7.327246665954591e-06\n",
            "Average reward: 0.848 Number of steps:  2.708 Training time:  6.811092853546143e-06\n",
            "Average reward: 0.832 Number of steps:  2.878 Training time:  7.06781816482544e-06\n",
            "Average reward: 0.832 Number of steps:  2.722 Training time:  6.694382190704346e-06\n",
            "Average reward: 0.836 Number of steps:  2.714 Training time:  6.730966091156006e-06\n",
            "Average reward: 0.852 Number of steps:  2.772 Training time:  6.954862117767334e-06\n",
            "Average reward: 0.828 Number of steps:  2.68 Training time:  6.696374416351318e-06\n",
            "Average reward: 0.832 Number of steps:  2.754 Training time:  6.781225204467774e-06\n",
            "Average reward: 0.868 Number of steps:  2.66 Training time:  6.580150604248047e-06\n",
            "Average reward: 0.872 Number of steps:  2.71 Training time:  6.557325363159179e-06\n",
            "Average reward: 0.868 Number of steps:  2.62 Training time:  6.50181531906128e-06\n",
            "Average reward: 0.816 Number of steps:  2.588 Training time:  6.535459518432616e-06\n",
            "Average reward: 0.868 Number of steps:  2.682 Training time:  6.638805866241456e-06\n",
            "Average reward: 0.852 Number of steps:  2.676 Training time:  6.617241382598877e-06\n",
            "Average reward: 0.88 Number of steps:  2.646 Training time:  6.631338119506835e-06\n",
            "Average reward: 0.82 Number of steps:  2.604 Training time:  6.495144367218017e-06\n",
            "Average reward: 0.848 Number of steps:  2.804 Training time:  6.944672584533691e-06\n",
            "Average reward: 0.892 Number of steps:  2.58 Training time:  6.444111824035644e-06\n",
            "Average reward: 0.832 Number of steps:  2.772 Training time:  6.7806668281555175e-06\n",
            "Average reward: 0.88 Number of steps:  2.534 Training time:  6.4693579673767085e-06\n",
            "Average reward: 0.884 Number of steps:  2.558 Training time:  6.7692694664001465e-06\n",
            "Average reward: 0.844 Number of steps:  2.642 Training time:  6.775720596313477e-06\n",
            "Average reward: 0.86 Number of steps:  2.706 Training time:  6.889804840087891e-06\n",
            "Average reward: 0.888 Number of steps:  2.576 Training time:  6.759684562683105e-06\n",
            "Average reward: 0.852 Number of steps:  2.67 Training time:  6.821360111236572e-06\n",
            "Average reward: 0.904 Number of steps:  2.684 Training time:  6.7756295204162594e-06\n",
            "Average reward: 0.88 Number of steps:  2.726 Training time:  6.935029983520507e-06\n",
            "Average reward: 0.872 Number of steps:  2.584 Training time:  6.573166847229004e-06\n",
            "Average reward: 0.888 Number of steps:  2.658 Training time:  6.665929794311524e-06\n",
            "Average reward: 0.864 Number of steps:  2.648 Training time:  6.64359474182129e-06\n",
            "Average reward: 0.868 Number of steps:  2.826 Training time:  7.237059593200683e-06\n",
            "Average reward: 0.856 Number of steps:  2.566 Training time:  6.546977519989013e-06\n",
            "Average reward: 0.924 Number of steps:  2.772 Training time:  7.121156692504882e-06\n",
            "Average reward: 0.888 Number of steps:  2.674 Training time:  6.8286528587341305e-06\n",
            "Average reward: 0.912 Number of steps:  2.512 Training time:  6.389187335968018e-06\n",
            "Average reward: 0.844 Number of steps:  2.656 Training time:  6.63746166229248e-06\n",
            "Average reward: 0.864 Number of steps:  2.878 Training time:  7.054802417755126e-06\n",
            "Average reward: 0.904 Number of steps:  2.668 Training time:  6.506037712097168e-06\n",
            "Average reward: 0.888 Number of steps:  2.608 Training time:  6.490242958068846e-06\n",
            "Average reward: 0.86 Number of steps:  2.622 Training time:  6.521990776062011e-06\n",
            "Average reward: 0.876 Number of steps:  2.682 Training time:  6.6242580413818365e-06\n",
            "Average reward: 0.876 Number of steps:  2.572 Training time:  6.345626354217529e-06\n",
            "Average reward: 0.856 Number of steps:  2.616 Training time:  6.48783540725708e-06\n",
            "Average reward: 0.872 Number of steps:  2.688 Training time:  6.697113990783692e-06\n",
            "Average reward: 0.852 Number of steps:  2.666 Training time:  6.580788135528565e-06\n",
            "Average reward: 0.928 Number of steps:  2.614 Training time:  6.613926410675049e-06\n",
            "Average reward: 0.924 Number of steps:  2.482 Training time:  6.215724468231201e-06\n",
            "Average reward: 0.932 Number of steps:  2.646 Training time:  6.456357955932617e-06\n",
            "Average reward: 0.904 Number of steps:  2.576 Training time:  6.3408513069152825e-06\n",
            "Average reward: 0.896 Number of steps:  2.578 Training time:  6.3225107192993165e-06\n",
            "Average reward: 0.908 Number of steps:  2.574 Training time:  6.4491796493530274e-06\n",
            "Average reward: 0.924 Number of steps:  2.548 Training time:  6.206920623779297e-06\n",
            "Average reward: 0.912 Number of steps:  2.732 Training time:  6.6885085105895995e-06\n",
            "Average reward: 0.904 Number of steps:  2.672 Training time:  6.715060234069824e-06\n",
            "Average reward: 0.876 Number of steps:  2.654 Training time:  6.547288417816162e-06\n",
            "Average reward: 0.892 Number of steps:  2.644 Training time:  6.611783027648926e-06\n",
            "Average reward: 0.912 Number of steps:  2.614 Training time:  6.468751907348633e-06\n",
            "Average reward: 0.876 Number of steps:  2.552 Training time:  6.34004545211792e-06\n",
            "Average reward: 0.916 Number of steps:  2.65 Training time:  6.4466423988342284e-06\n",
            "Average reward: 0.896 Number of steps:  2.532 Training time:  6.351839065551758e-06\n",
            "Average reward: 0.908 Number of steps:  2.522 Training time:  6.18363618850708e-06\n",
            "Average reward: 0.904 Number of steps:  2.548 Training time:  6.3695726394653315e-06\n",
            "Average reward: 0.88 Number of steps:  2.562 Training time:  6.36546516418457e-06\n",
            "Average reward: 0.924 Number of steps:  2.562 Training time:  6.223749160766602e-06\n",
            "Average reward: 0.896 Number of steps:  2.682 Training time:  6.6746811866760254e-06\n",
            "Average reward: 0.884 Number of steps:  2.53 Training time:  6.372547149658203e-06\n",
            "Average reward: 0.904 Number of steps:  2.404 Training time:  5.908644676208497e-06\n",
            "Average reward: 0.928 Number of steps:  2.416 Training time:  5.9636878967285156e-06\n",
            "Average reward: 0.936 Number of steps:  2.378 Training time:  5.851578235626221e-06\n",
            "Average reward: 0.908 Number of steps:  2.492 Training time:  6.20269775390625e-06\n",
            "Average reward: 0.948 Number of steps:  2.594 Training time:  6.32533073425293e-06\n",
            "Average reward: 0.888 Number of steps:  2.52 Training time:  6.173337459564209e-06\n",
            "Average reward: 0.94 Number of steps:  2.436 Training time:  6.002862453460693e-06\n",
            "Average reward: 0.9 Number of steps:  2.602 Training time:  6.370121002197266e-06\n",
            "Average reward: 0.924 Number of steps:  2.37 Training time:  5.849696159362794e-06\n",
            "Average reward: 0.924 Number of steps:  2.462 Training time:  6.051785945892334e-06\n",
            "Average reward: 0.916 Number of steps:  2.512 Training time:  6.198397636413574e-06\n",
            "Average reward: 0.908 Number of steps:  2.534 Training time:  6.3350224494934085e-06\n",
            "Average reward: 0.904 Number of steps:  2.502 Training time:  6.1786861419677735e-06\n",
            "Average reward: 0.928 Number of steps:  2.61 Training time:  6.560196876525878e-06\n",
            "Average reward: 0.888 Number of steps:  2.512 Training time:  6.274533271789551e-06\n",
            "Average reward: 0.932 Number of steps:  2.498 Training time:  6.1971659660339365e-06\n",
            "Average reward: 0.848 Number of steps:  2.456 Training time:  6.034902572631836e-06\n",
            "Average reward: 0.908 Number of steps:  2.512 Training time:  6.2131376266479495e-06\n",
            "Average reward: 0.884 Number of steps:  2.494 Training time:  6.239222526550294e-06\n",
            "Average reward: 0.948 Number of steps:  2.48 Training time:  6.70411729812622e-06\n",
            "Average reward: 0.936 Number of steps:  2.522 Training time:  6.505871772766113e-06\n",
            "Average reward: 0.908 Number of steps:  2.496 Training time:  6.280938625335694e-06\n",
            "Average reward: 0.94 Number of steps:  2.41 Training time:  6.306273937225342e-06\n",
            "Average reward: 0.94 Number of steps:  2.646 Training time:  6.873032569885254e-06\n",
            "Average reward: 0.94 Number of steps:  2.238 Training time:  5.734975814819336e-06\n",
            "Average reward: 0.92 Number of steps:  2.518 Training time:  6.3684163093566894e-06\n",
            "Average reward: 0.92 Number of steps:  2.642 Training time:  6.787707805633545e-06\n",
            "Average reward: 0.924 Number of steps:  2.576 Training time:  6.498642444610596e-06\n",
            "Average reward: 0.956 Number of steps:  2.668 Training time:  6.88993501663208e-06\n",
            "Average reward: 0.924 Number of steps:  2.526 Training time:  6.519088745117188e-06\n",
            "Average reward: 0.92 Number of steps:  2.572 Training time:  6.636670112609863e-06\n",
            "Average reward: 0.912 Number of steps:  2.556 Training time:  6.644226074218751e-06\n",
            "Average reward: 0.948 Number of steps:  2.438 Training time:  6.379290103912354e-06\n",
            "Average reward: 0.928 Number of steps:  2.476 Training time:  6.374333858489991e-06\n",
            "Average reward: 0.908 Number of steps:  2.558 Training time:  6.427601337432861e-06\n",
            "Average reward: 0.944 Number of steps:  2.348 Training time:  5.792699813842773e-06\n",
            "Average reward: 0.932 Number of steps:  2.356 Training time:  5.941641330718994e-06\n",
            "Average reward: 0.92 Number of steps:  2.544 Training time:  6.299431324005127e-06\n",
            "Average reward: 0.944 Number of steps:  2.558 Training time:  6.402662754058838e-06\n",
            "Average reward: 0.924 Number of steps:  2.412 Training time:  5.943554878234864e-06\n",
            "Average reward: 0.916 Number of steps:  2.488 Training time:  6.183876991271973e-06\n",
            "Average reward: 0.916 Number of steps:  2.408 Training time:  6.0024271011352545e-06\n",
            "Average reward: 0.928 Number of steps:  2.402 Training time:  6.008630752563476e-06\n",
            "Average reward: 0.92 Number of steps:  2.546 Training time:  6.3538179397583e-06\n",
            "Average reward: 0.916 Number of steps:  2.47 Training time:  6.088459491729736e-06\n",
            "Average reward: 0.932 Number of steps:  2.308 Training time:  5.767452716827392e-06\n",
            "Average reward: 0.916 Number of steps:  2.442 Training time:  6.283758640289306e-06\n",
            "Average reward: 0.948 Number of steps:  2.51 Training time:  6.494398117065431e-06\n",
            "Average reward: 0.948 Number of steps:  2.48 Training time:  6.247151851654053e-06\n",
            "Average reward: 0.94 Number of steps:  2.494 Training time:  6.278696537017823e-06\n",
            "Average reward: 0.92 Number of steps:  2.422 Training time:  6.088341236114502e-06\n",
            "Average reward: 0.948 Number of steps:  2.464 Training time:  6.346299648284912e-06\n",
            "Average reward: 0.928 Number of steps:  2.366 Training time:  5.989234924316406e-06\n",
            "Average reward: 0.948 Number of steps:  2.42 Training time:  6.046100616455079e-06\n",
            "Average reward: 0.932 Number of steps:  2.6 Training time:  6.401654720306396e-06\n",
            "Average reward: 0.916 Number of steps:  2.486 Training time:  6.17612886428833e-06\n",
            "Average reward: 0.904 Number of steps:  2.466 Training time:  6.1148209571838375e-06\n",
            "Average reward: 0.92 Number of steps:  2.402 Training time:  6.104139328002929e-06\n",
            "Average reward: 0.936 Number of steps:  2.476 Training time:  6.144481182098389e-06\n",
            "Average reward: 0.92 Number of steps:  2.39 Training time:  5.916023254394531e-06\n",
            "Average reward: 0.912 Number of steps:  2.496 Training time:  6.09380054473877e-06\n",
            "Average reward: 0.964 Number of steps:  2.458 Training time:  6.09857702255249e-06\n",
            "Average reward: 0.948 Number of steps:  2.4 Training time:  5.823014736175537e-06\n",
            "Average reward: 0.96 Number of steps:  2.45 Training time:  6.134176254272461e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PRINT STEPS OF AN EPISODE CONSIDERING THE TRAINED AGENT\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()                       # INTIALISE GAME\n",
        "\n",
        "print(S)                                                  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)\n",
        "\n",
        "print('check? ',env.check)                                # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)\n",
        "print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))    # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    \n",
        "    a_agent = agent.choose_action(X, allowed_a, 0)\n",
        "\n",
        "    S,X,allowed_a,R,Done=env.OneStep(a_agent)   # UPDATE THE ENVIRONMENT\n",
        "    \n",
        "    \n",
        "    ## PRINT CHESS BOARD AND VARIABLES\n",
        "    print('')\n",
        "    print(S)\n",
        "    print(R,'', Done)\n",
        "    print('check? ',env.check)\n",
        "    print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))\n",
        "    \n",
        "    \n",
        "    # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)\n",
        "    if Done:\n",
        "        break\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YdLivsyN3R9",
        "outputId": "c0962646-1a19-4c79-a51f-b9b9006ac49b"
      },
      "id": "2YdLivsyN3R9",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0]\n",
            " [3 0 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 2]]\n",
            "check?  0\n",
            "dofk2  3\n",
            "\n",
            "[[3 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 1]\n",
            " [0 2 0 0]]\n",
            "0  0\n",
            "check?  0\n",
            "dofk2  1\n",
            "\n",
            "[[0 0 0 0]\n",
            " [3 0 1 0]\n",
            " [0 0 0 0]\n",
            " [0 2 0 0]]\n",
            "0  0\n",
            "check?  0\n",
            "dofk2  1\n",
            "\n",
            "[[0 0 0 0]\n",
            " [3 2 1 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n",
            "1  1\n",
            "check?  1\n",
            "dofk2  0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Copy of Assignment (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}